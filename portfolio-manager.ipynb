{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import copy\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv():\n",
    "    def __init__(self, dates, datasets, n):\n",
    "        self.dates = dates\n",
    "        self.datasets = datasets\n",
    "        self.n = n\n",
    "        self.process_big_pt()\n",
    "        \n",
    "        self.initial_pt = 1000000 # 1 000 000\n",
    "        self.c_minus = 0.0025 # 0.25%\n",
    "        self.c_plus = 0.0025 # 0.25%\n",
    "        self.delta = 10000 # 10 000\n",
    "                \n",
    "        self.process_actions()\n",
    "        self.action_shape = self.actions.shape[0]\n",
    "        self._episode_ended = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pt = self.initial_pt # 1 000 000\n",
    "        self.wt = [0.25,0.25,0.25,0.25]\n",
    "        \n",
    "        self.current_tick = 1\n",
    "        self.episode_ended = False\n",
    "        \n",
    "        return {'big_xt':np.array(self.big_pt[0,:,:]), 'wt_prime':self.wt}\n",
    "    \n",
    "    def process_actions(self):\n",
    "        asset_number = 3\n",
    "        action_number = 3\n",
    "\n",
    "        seq = np.arange(asset_number)\n",
    "        actions = []\n",
    "\n",
    "        for c in product(seq, repeat=action_number):\n",
    "            actions.append(c)\n",
    "\n",
    "        self.actions = np.array(actions)\n",
    "    \n",
    "    def find_action_index(self,action):\n",
    "        for ind, a in enumerate(self.actions):\n",
    "            if np.array_equal(a, action):\n",
    "                return ind;\n",
    "    \n",
    "    def process_big_pt(self):\n",
    "        datasets = self.datasets\n",
    "        date_start = self.dates[0]\n",
    "        date_end = self.dates[1]\n",
    "\n",
    "        dfs = []\n",
    "        for d in datasets:\n",
    "            ticker = yf.Ticker(d)\n",
    "            # get historical market data\n",
    "            df_ = ticker.history(start=date_start, end=date_end, interval=\"1d\")    \n",
    "            df_.rename(mapper={\n",
    "                \"Close\": d+\"_close\",\n",
    "                \"Open\": d+\"_open\",\n",
    "                \"High\": d+\"_high\",\n",
    "                \"Low\": d+\"_low\",\n",
    "                \"Volume\": d+\"_volume\"\n",
    "            }, inplace=True, axis=1)\n",
    "            if \"Dividends\" in df_.columns:\n",
    "                df_.drop(axis=1,labels=[\"Dividends\", \"Stock Splits\"],inplace=True)\n",
    "            dfs.append(df_)\n",
    "\n",
    "        final_df = pd.concat(dfs, axis=1)\n",
    "        final_df.dropna(inplace=True)\n",
    "        self.final_df = final_df\n",
    "        \n",
    "        final_df = self.final_df\n",
    "        n = self.n\n",
    "        Pc = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_prev_close = final_df[d+\"_close\"].shift().values\n",
    "            Kc = (asset_close - asset_prev_close) / asset_prev_close\n",
    "            Kc = Kc[1:]\n",
    "            Pc_ = [Kc[i:i+n] for i in range(len(asset_close)-n)]\n",
    "            Pc.append(Pc_)\n",
    "        Pc = np.array(Pc)\n",
    "        Po = []\n",
    "        for d in datasets:\n",
    "            asset_prev_close = final_df[d+\"_close\"].shift().values\n",
    "            asset_open = final_df[d+\"_open\"].values\n",
    "            Ko = (asset_open - asset_prev_close) / asset_prev_close\n",
    "            Ko = Ko[1:]\n",
    "            Po_ = [Ko[i:i+n] for i in range(len(asset_open)-n)]\n",
    "            Po.append(Po_)\n",
    "        Po = np.array(Po)\n",
    "        Pl = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_low = final_df[d+\"_low\"].values\n",
    "            Kl = (asset_close - asset_low) / asset_low\n",
    "            Kl = Kl[1:]\n",
    "            Pl_ = [Kl[i:i+n] for i in range(len(asset_low)-n)]\n",
    "            Pl.append(Pl_)\n",
    "        Pl = np.array(Pl)\n",
    "        Ph = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_high = final_df[d+\"_high\"].values\n",
    "            Kh = (asset_close - asset_high) / asset_high\n",
    "            Kh = Kh[1:]\n",
    "            Ph_ = [Kh[i:i+n] for i in range(len(asset_high)-n)]\n",
    "            Ph.append(Ph_)\n",
    "        Ph = np.array(Ph)\n",
    "        Pv = []\n",
    "        for d in datasets:\n",
    "            asset_prev_volume = final_df[d+\"_volume\"].shift().values\n",
    "            asset_volume = final_df[d+\"_volume\"].values\n",
    "            Kv = (asset_volume - asset_prev_volume) / asset_prev_volume\n",
    "            Kv = Kv[1:]\n",
    "            Pv_ = [Kv[i:i+n] for i in range(len(asset_high)-n)]\n",
    "            Pv.append(Pv_)\n",
    "        Pv = np.array(Pv)\n",
    "        Pt_star = np.array([Pc, Po, Pl, Ph, Pv])\n",
    "\n",
    "        self.big_pt = Pt_star.swapaxes(0,2).swapaxes(1,2)\n",
    "        print(self.big_pt.shape)\n",
    "    \n",
    "    def phi(self,v):\n",
    "        return np.insert(v, 0, [0]) + np.ones(len(v) + 1)\n",
    "    \n",
    "    def get_wt_prime_chapeau(self,wt_prime,big_s_minus,big_s_plus,pt_prime):\n",
    "        wt_prime_chapeau = []\n",
    "        for ind, key in enumerate(wt_prime):\n",
    "            if(ind in big_s_minus):\n",
    "                wt_prime_chapeau.append(key - self.delta/pt_prime)\n",
    "            elif(ind in big_s_plus):\n",
    "                wt_prime_chapeau.append(key + self.delta/pt_prime)\n",
    "            else:\n",
    "                wt_prime_chapeau.append(key)\n",
    "    \n",
    "        return np.array(wt_prime_chapeau)\n",
    "    \n",
    "    def is_asset_shortage(self,action,pt,wt):\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        for ind in big_s_minus:\n",
    "            if wt[ind+1]*pt < self.delta:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def is_cash_shortage(self,action,pt,wt):\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        current_cash = wt[0]*pt\n",
    "        cash_after_selling = current_cash + (1-self.c_minus)*self.delta*len(big_s_minus) # must include transaction costs\n",
    "        cash_needed = (self.c_plus+1)*self.delta*len(big_s_plus) # must include transaction costs\n",
    "        \n",
    "        if(cash_after_selling < cash_needed):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def action_mapping(self,action,action_Q_values,pt,wt):\n",
    "        action = copy(action)\n",
    "        action_mapped = action\n",
    "        if self.is_asset_shortage(action,pt,wt):\n",
    "            action_mapped = self.rule2(action,action_Q_values,pt,wt)\n",
    "        elif self.is_cash_shortage(action,pt,wt):\n",
    "            action_mapped = self.rule1(action,action_Q_values,pt,wt)\n",
    "        \n",
    "        return action_mapped\n",
    "    \n",
    "    def rule1(self,action,action_Q_values,pt,wt):\n",
    "        MAXQ = np.NINF\n",
    "        action_selected = action\n",
    "        \n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        \n",
    "        for i in range(1,len(big_s_plus) + 1):\n",
    "            for c in combinations(big_s_plus, i):\n",
    "                new_action = copy(action)\n",
    "                for j in c:\n",
    "                    new_action[j] = 1\n",
    "\n",
    "                if not self.is_cash_shortage(new_action,pt,wt):\n",
    "                    new_action_index = self.find_action_index(new_action)\n",
    "                    new_action_Q_value = action_Q_values[new_action_index]\n",
    "                    try:\n",
    "                        if new_action_Q_value > MAXQ:\n",
    "                            MAXQ = new_action_Q_value\n",
    "                            action_selected = new_action\n",
    "                    except ValueError:\n",
    "                        print(':::::: ERROR ::::::')\n",
    "                        print(action_Q_values.shape)\n",
    "                        print(new_action_index)\n",
    "                        print(new_action)\n",
    "                        print(new_action.shape)\n",
    "                        print(self.actions)\n",
    "        \n",
    "        return action_selected\n",
    "    \n",
    "    def rule2(self,action,action_Q_values,pt,wt):\n",
    "        for i in range(len(action)):\n",
    "            if wt[i+1]*pt < self.delta:\n",
    "                action[i] = 1\n",
    "        \n",
    "        if self.is_cash_shortage(action,pt,wt):\n",
    "            action = self.rule1(action,action_Q_values,pt,wt)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def F(self,pt,wt):\n",
    "        action_possible = []\n",
    "        for ind, action in enumerate(self.actions):\n",
    "            if not self.is_asset_shortage(action,pt,wt) and not self.is_cash_shortage(action,pt,wt):\n",
    "                action_possible.append(ind)\n",
    "\n",
    "        return np.array(action_possible)\n",
    "    \n",
    "    def step(self, action, simulation=False):\n",
    "        # Must set new portfolio with regards to action\n",
    "        # Must set new reward\n",
    "        if self.current_tick == len(self.big_pt) - 2:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            self.episode_ended = True\n",
    "        \n",
    "        ktc = self.big_pt[self.current_tick,0,:,9] # nouveaux close prices des différents assets\n",
    "        pt_prime = self.pt * np.dot(self.wt,self.phi(ktc)) # nouvelle valeur du portfolio issue de l'action précédente\n",
    "        \n",
    "        wt_prime = (self.wt*self.phi(ktc)) / (np.dot(self.wt,self.phi(ktc))) # nouvelles proportions des assets du portfolio\n",
    "        \n",
    "        # On prend en compte la nouvelle action\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        \n",
    "        ct = (self.delta*(self.c_minus*len(big_s_minus) + self.c_plus*len(big_s_plus)))/pt_prime\n",
    "        if not simulation:\n",
    "            self.pt = pt_prime*(1 - ct)\n",
    "        else:\n",
    "            pt = pt_prime*(1 - ct)\n",
    "        \n",
    "        wt_prime_chapeau_1tillend = self.get_wt_prime_chapeau(wt_prime[1:],big_s_minus,big_s_plus,pt_prime)\n",
    "        wt_prime_chapeau_0 = wt_prime[0] + self.delta*((1-self.c_minus)*len(big_s_minus)-(1+self.c_plus)*len(big_s_plus))/pt_prime\n",
    "        wt_prime_chapeau = np.concatenate((np.array([wt_prime_chapeau_0]), wt_prime_chapeau_1tillend))\n",
    "        \n",
    "        if not simulation:\n",
    "            self.wt = wt_prime_chapeau / (np.dot(wt_prime_chapeau, np.ones(len(wt_prime_chapeau))))\n",
    "            self.current_tick += 1\n",
    "            k_t_plus_one_c = self.big_pt[self.current_tick,0,:,9]\n",
    "        else:\n",
    "            wt = wt_prime_chapeau / (np.dot(wt_prime_chapeau, np.ones(len(wt_prime_chapeau))))\n",
    "            current_tick = self.current_tick + 1\n",
    "            k_t_plus_one_c = self.big_pt[current_tick,0,:,9]\n",
    "        \n",
    "        big_p_s_t_plus_one = pt_prime*np.dot(wt_prime, self.phi(k_t_plus_one_c))\n",
    "        \n",
    "        if not simulation:\n",
    "            p_t_plus_one_prime = self.pt * np.dot(self.wt,self.phi(k_t_plus_one_c))\n",
    "            reward = (p_t_plus_one_prime - big_p_s_t_plus_one)/big_p_s_t_plus_one\n",
    "            wt_plus_one_prime = (self.wt*self.phi(k_t_plus_one_c)) / (np.dot(self.wt,self.phi(k_t_plus_one_c)))\n",
    "        else:\n",
    "            p_t_plus_one_prime = pt * np.dot(wt,self.phi(k_t_plus_one_c))\n",
    "            reward = (p_t_plus_one_prime - big_p_s_t_plus_one)/big_p_s_t_plus_one\n",
    "            wt_plus_one_prime = (wt*self.phi(k_t_plus_one_c)) / (np.dot(wt,self.phi(k_t_plus_one_c)))\n",
    "        \n",
    "        if not simulation:\n",
    "            return {'big_xt':np.array(self.big_pt[self.current_tick]), 'wt_prime':wt_plus_one_prime}, reward, self.episode_ended\n",
    "        else:\n",
    "            return {'big_xt':np.array(self.big_pt[current_tick]), 'wt_prime':wt_plus_one_prime}, reward, self.episode_ended, pt, wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1751, 5, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"SPY\", \"IWD\", \"IWC\"]\n",
    "train_dates = [\"2010-01-01\", \"2016-12-30\"]\n",
    "n = 10\n",
    "env = PortfolioEnv(train_dates, datasets, 10)\n",
    "datas_ = env.big_pt\n",
    "datas = datas_.swapaxes(2,3).swapaxes(1,3)\n",
    "final_datas = []\n",
    "for d in datas:\n",
    "    final_datas.append(d[0])\n",
    "    final_datas.append(d[1])\n",
    "    final_datas.append(d[2])\n",
    "final_datas = np.array(final_datas)\n",
    "mm_scaler = MinMaxScaler()\n",
    "datas_scaled = np.array([mm_scaler.fit_transform(d) for d in final_datas]) # MinMaxScaler par ligne ou par colonne ?\n",
    "X_train = datas_scaled[:4500]\n",
    "X_valid = datas_scaled[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units=128, input_shape=[10, 5], return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=10),\n",
    "])\n",
    "recurrent_decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.RepeatVector(10, input_shape=[10]),\n",
    "    tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation=\"sigmoid\"))\n",
    "])\n",
    "recurrent_ae = tf.keras.models.Sequential([recurrent_encoder, recurrent_decoder])\n",
    "recurrent_ae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "recurrent_encoder.summary()\n",
    "recurrent_decoder.summary()\n",
    "recurrent_ae.summary()\n",
    "\n",
    "history = recurrent_ae.fit(X_train, X_train, epochs=40, validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 5, 3, 10)\n",
      "(241, 5, 3, 10)\n",
      "(239, 5, 3, 10)\n",
      "(240, 5, 3, 10)\n",
      "(240, 5, 3, 10)\n",
      "(240, 5, 3, 10)\n",
      "(241, 5, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "def create_envs(dates, datasets, n):\n",
    "    return np.array([PortfolioEnv(d, datasets, n) for d in dates])\n",
    "datasets = [\"SPY\", \"IWD\", \"IWC\"]\n",
    "train_dates = [\n",
    "    [\"2010-01-01\", \"2010-12-30\"],\n",
    "    [\"2011-01-01\", \"2011-12-30\"],\n",
    "    [\"2012-01-01\", \"2012-12-30\"],\n",
    "    [\"2013-01-01\", \"2013-12-30\"],\n",
    "    [\"2014-01-01\", \"2014-12-30\"],\n",
    "    [\"2015-01-01\", \"2015-12-30\"],\n",
    "    [\"2016-01-01\", \"2016-12-30\"],\n",
    "]\n",
    "n = 10\n",
    "train_envs = create_envs(train_dates, datasets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(preprocessed_states, mask, target_Q_values):\n",
    "    with tf.device('gpu:0'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = model(preprocessed_states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .3\n",
    "epochs = 500 \n",
    "\n",
    "bins = np.arange(0, 20, 1)\n",
    "r = np.random.rand(epochs)\n",
    "N=len(train_envs)\n",
    "gen_trunc=(N-1-np.floor(np.log(1-r*(1-(1-beta)**N))/np.log(1-beta))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "K = tf.keras.backend\n",
    "\n",
    "input_states = tf.keras.layers.Input(shape=[34])\n",
    "hidden1 = tf.keras.layers.Dense(64, activation=\"relu\")(input_states)\n",
    "hidden2 = tf.keras.layers.Dense(32, activation=\"relu\")(hidden1)\n",
    "output = tf.keras.layers.Dense(train_envs[0].action_shape)(hidden2)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_states], outputs=[output])\n",
    "\n",
    "target = tf.keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state, concatenate=True): # works when only one state, but not with several states\n",
    "    big_xt = state['big_xt']\n",
    "    big_xt = big_xt.swapaxes(0,1).swapaxes(1,2)\n",
    "    big_xt_scaled = np.array([mm_scaler.fit_transform(a) for a in big_xt])\n",
    "\n",
    "    predictions = recurrent_encoder.predict(big_xt_scaled)\n",
    "    predictions_reshaped = predictions.reshape((big_xt_scaled.shape[0]*big_xt_scaled.shape[1]))\n",
    "    \n",
    "    if(concatenate):\n",
    "        predictions_reshaped = np.concatenate((predictions_reshaped, state['wt_prime']))\n",
    "    \n",
    "    return predictions_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "epsilon = 1\n",
    "batch_size = 16\n",
    "discount_rate = .9\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-7)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "steps = 0\n",
    "episode_counter = 1\n",
    "\n",
    "train_CR = []\n",
    "previous_CR = 0\n",
    "\n",
    "for ind in gen_trunc:\n",
    "    env = train_envs[ind]\n",
    "    state = env.reset()\n",
    "        \n",
    "    pbar = tqdm(total=len(env.big_pt)-1)\n",
    "    \n",
    "    epsilon = max(1 - episode_counter / 400, 0.01)\n",
    "    \n",
    "    while 1: # will break when end of env\n",
    "        pbar.set_description(f'episode {episode_counter}/{len(gen_trunc)} - previous CR:{previous_CR}')\n",
    "        #epsilon-greedy policy to select an action\n",
    "        action = None\n",
    "        state_preprocessed = preprocess_state(state)\n",
    "        if np.random.rand() < epsilon: # goal is to take uniformly random action among possible actions of this env\n",
    "            possible_actions = env.F(env.pt, env.wt)\n",
    "            random_index = np.random.randint(len(possible_actions))\n",
    "            action = env.actions[possible_actions[random_index]]\n",
    "        else: # here goal is to make model predict best action, but if action isn't feasable for env, map it\n",
    "            Q_values = model.predict(state_preprocessed[np.newaxis])\n",
    "            action = env.action_mapping(env.actions[np.argmax(Q_values[0])], Q_values[0], env.pt, env.wt)\n",
    "        \n",
    "        # simulate all feasible actions for current state\n",
    "        possible_actions = env.F(env.pt, env.wt)\n",
    "        simulations = []\n",
    "        \n",
    "        next_state_simulated_preprocessed_partial = None\n",
    "        is_next_state_simulated_preprocessed_partial = False\n",
    "        for simulated_action in possible_actions:\n",
    "            next_state_simulated, next_reward, done, next_pt, next_wt = env.step(env.actions[simulated_action], simulation=True)\n",
    "            if not is_next_state_simulated_preprocessed_partial:\n",
    "                next_state_simulated_preprocessed_partial = preprocess_state(next_state_simulated, concatenate=False)\n",
    "                is_next_state_simulated_preprocessed_partial = True\n",
    "                \n",
    "            next_state_simulated_preprocessed = np.concatenate((next_state_simulated['wt_prime'], next_state_simulated_preprocessed_partial))\n",
    "            simulations.append((state_preprocessed, simulated_action, next_reward, next_state_simulated_preprocessed, done, next_pt, next_wt))\n",
    "        replay_memory.append(np.array(simulations))\n",
    "        next_state, reward, episode_ended = env.step(action)\n",
    "        # do some stuff with it\n",
    "        \n",
    "        # add some time for buffer to populate\n",
    "        \n",
    "        if steps >= 50:\n",
    "            # start batch training (room to improvment, why don't use Prioritized Experience Replay ?)\n",
    "            # select a uniformly random batch\n",
    "            indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "            batch = [replay_memory[index] for index in indices]\n",
    "            for simulations in batch:\n",
    "                #start = time.time()\n",
    "                preprocessed_states, actions, rewards, preprocessed_next_states, dones, next_pts, next_wts = [np.array([simulation[field_index] for simulation in simulations]) for field_index in range(7)]\n",
    "                \n",
    "                next_Q_values = target.predict(preprocessed_next_states) # Q-values predicted for all next_states\n",
    "                max_next_Q_values_index = np.argmax(next_Q_values, axis=1) # for each next_states find index of max Q-value predicted by target\n",
    "                \n",
    "                # okay, here, in max_next_Q_values, there are some actions that may not be feasible and that we need to map\n",
    "                # but we have to use is_asset_shortage, is_cash_shortage and action_mapping with state of simulation\n",
    "                max_next_Q_values_actions = np.array([\n",
    "                    env.actions[action_simulated] if not env.is_asset_shortage(env.actions[action_simulated], next_pts[ind], next_wts[ind]) and not env.is_cash_shortage(env.actions[action_simulated], next_pts[ind], next_wts[ind])\n",
    "                    else env.action_mapping(env.actions[action_simulated], next_Q_values[ind], next_pts[ind], next_wts[ind])\n",
    "                    for ind, action_simulated in enumerate(max_next_Q_values_index)\n",
    "                ]) # here mistake\n",
    "                \n",
    "                # okay, we have the feasible actions selected by the target network, now we need to find their indexes to communicate with the network\n",
    "                max_next_Q_values_indexes = [env.find_action_index(action_simulated) for action_simulated in max_next_Q_values_actions]\n",
    "                # now we have to find the Q-values predicted by target network of these corrected actions\n",
    "                max_next_Q_values = [next_Q_values[ind][best_action_simulated] for ind, best_action_simulated in enumerate(max_next_Q_values_indexes)]\n",
    "\n",
    "                # and now, we can proprely calculate the target Q-value\n",
    "                target_Q_values = (rewards +\n",
    "                                   (1 - dones) * discount_rate * max_next_Q_values) # here problem with dones /!\\\n",
    "                target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "                mask = tf.one_hot(actions, env.action_shape)\n",
    "                \n",
    "                train_step(preprocessed_states, mask, target_Q_values)\n",
    "                # and we're done, amazing !!\n",
    "                #print('updates ::: {:.5f} ms / step'.format((time.time() - start) * 1000))\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "        steps += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if episode_ended:\n",
    "            break;\n",
    "        \n",
    "    episode_counter += 1\n",
    "    \n",
    "    previous_CR = round((env.pt-env.initial_pt)/env.initial_pt, 3)\n",
    "    train_CR.append(previous_CR)\n",
    "    pbar.close()\n",
    "    # update of target network\n",
    "    target.set_weights(model.get_weights()) "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
