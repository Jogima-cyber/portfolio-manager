{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import copy\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv():\n",
    "    def __init__(self, dates, datasets, n, encoder=None):\n",
    "        self.dates = dates\n",
    "        self.datasets = datasets\n",
    "        self.n = n\n",
    "        self.process_big_pt()\n",
    "        \n",
    "        if encoder is not None:\n",
    "            self.process_small_pt(encoder)\n",
    "        \n",
    "        self.initial_pt = 1000000 # 1 000 000\n",
    "        self.c_minus = 0.0025 # 0.25%\n",
    "        self.c_plus = 0.0025 # 0.25%\n",
    "        self.delta = 10000 # 10 000\n",
    "                \n",
    "        self.process_actions()\n",
    "        self.action_shape = self.actions.shape[0]\n",
    "        self._episode_ended = False\n",
    "    \n",
    "    def reset(self):\n",
    "        # initialisation of portfolio\n",
    "        self.pt = self.initial_pt # 1 000 000\n",
    "        self.wt = [0.25,0.25,0.25,0.25]\n",
    "        \n",
    "        self.current_tick = 0 # after checking, current_tick should be set to 0\n",
    "        self.episode_ended = False\n",
    "        \n",
    "        # after checking, we should return state 0\n",
    "        return {'big_xt':np.array(self.small_pt[0]), 'wt_prime':self.wt}\n",
    "    \n",
    "    def process_actions(self):\n",
    "        asset_number = 3\n",
    "        action_number = 3\n",
    "\n",
    "        seq = np.arange(asset_number)\n",
    "        actions = []\n",
    "\n",
    "        for c in product(seq, repeat=action_number):\n",
    "            actions.append(c)\n",
    "\n",
    "        self.actions = np.array(actions)\n",
    "    \n",
    "    def find_action_index(self,action):\n",
    "        for ind, a in enumerate(self.actions):\n",
    "            if np.array_equal(a, action):\n",
    "                return ind;\n",
    "    \n",
    "    def process_big_pt(self):\n",
    "        datasets = self.datasets\n",
    "        date_start = self.dates[0]\n",
    "        date_end = self.dates[1]\n",
    "\n",
    "        dfs = []\n",
    "        for d in datasets:\n",
    "            ticker = yf.Ticker(d)\n",
    "            # get historical market data\n",
    "            df_ = ticker.history(start=date_start, end=date_end, interval=\"1d\")    \n",
    "            df_.rename(mapper={\n",
    "                \"Close\": d+\"_close\",\n",
    "                \"Open\": d+\"_open\",\n",
    "                \"High\": d+\"_high\",\n",
    "                \"Low\": d+\"_low\",\n",
    "                \"Volume\": d+\"_volume\"\n",
    "            }, inplace=True, axis=1)\n",
    "            if \"Dividends\" in df_.columns:\n",
    "                df_.drop(axis=1,labels=[\"Dividends\", \"Stock Splits\"],inplace=True)\n",
    "            dfs.append(df_)\n",
    "\n",
    "        final_df = pd.concat(dfs, axis=1)\n",
    "        final_df.dropna(inplace=True)\n",
    "        self.final_df = final_df\n",
    "        \n",
    "        final_df = self.final_df\n",
    "        n = self.n\n",
    "        Pc = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_prev_close = final_df[d+\"_close\"].shift().values\n",
    "            Kc = (asset_close - asset_prev_close) / asset_prev_close\n",
    "            Kc = Kc[1:]\n",
    "            Pc_ = [Kc[i:i+n] for i in range(len(asset_close)-n)] # Kc[0:20], Kc[1:21], Kc[2:22]\n",
    "            Pc.append(Pc_)\n",
    "        Pc = np.array(Pc)\n",
    "        Po = []\n",
    "        for d in datasets:\n",
    "            asset_prev_close = final_df[d+\"_close\"].shift().values\n",
    "            asset_open = final_df[d+\"_open\"].values\n",
    "            Ko = (asset_open - asset_prev_close) / asset_prev_close\n",
    "            Ko = Ko[1:]\n",
    "            Po_ = [Ko[i:i+n] for i in range(len(asset_open)-n)]\n",
    "            Po.append(Po_)\n",
    "        Po = np.array(Po)\n",
    "        Pl = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_low = final_df[d+\"_low\"].values\n",
    "            Kl = (asset_close - asset_low) / asset_low\n",
    "            Kl = Kl[1:]\n",
    "            Pl_ = [Kl[i:i+n] for i in range(len(asset_low)-n)]\n",
    "            Pl.append(Pl_)\n",
    "        Pl = np.array(Pl)\n",
    "        Ph = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_high = final_df[d+\"_high\"].values\n",
    "            Kh = (asset_close - asset_high) / asset_high\n",
    "            Kh = Kh[1:]\n",
    "            Ph_ = [Kh[i:i+n] for i in range(len(asset_high)-n)]\n",
    "            Ph.append(Ph_)\n",
    "        Ph = np.array(Ph)\n",
    "        Pv = []\n",
    "        for d in datasets:\n",
    "            asset_prev_volume = final_df[d+\"_volume\"].shift().values\n",
    "            asset_volume = final_df[d+\"_volume\"].values\n",
    "            Kv = (asset_volume - asset_prev_volume) / asset_prev_volume\n",
    "            Kv = Kv[1:]\n",
    "            Pv_ = [Kv[i:i+n] for i in range(len(asset_high)-n)]\n",
    "            Pv.append(Pv_)\n",
    "        Pv = np.array(Pv)\n",
    "        Pt_star = np.array([Pc, Po, Pl, Ph, Pv])\n",
    "\n",
    "        self.big_pt = Pt_star.swapaxes(0,2).swapaxes(1,2)\n",
    "        print(self.big_pt.shape)\n",
    "        \n",
    "    def process_small_pt(self, encoder):\n",
    "        big_pt = copy(self.big_pt)\n",
    "        small_pt = []\n",
    "        \n",
    "        for big_xt in big_pt:\n",
    "            big_xt = big_xt.swapaxes(0,1).swapaxes(1,2)\n",
    "            big_xt_scaled = np.array([mm_scaler.fit_transform(a) for a in big_xt])\n",
    "\n",
    "            predictions = encoder.predict(big_xt_scaled)\n",
    "            predictions_reshaped = predictions.reshape((big_xt_scaled.shape[0]*big_xt_scaled.shape[1]))\n",
    "            \n",
    "            small_pt.append(predictions_reshaped)\n",
    "        \n",
    "        self.small_pt = np.array(small_pt)\n",
    "    \n",
    "    def phi(self,v):\n",
    "        return np.insert(v, 0, [0]) + np.ones(len(v) + 1)\n",
    "    \n",
    "    def get_wt_prime_chapeau(self,wt_prime,big_s_minus,big_s_plus,pt_prime):\n",
    "        wt_prime_chapeau = []\n",
    "        for ind, key in enumerate(wt_prime):\n",
    "            if(ind in big_s_minus):\n",
    "                wt_prime_chapeau.append(key - self.delta/pt_prime)\n",
    "            elif(ind in big_s_plus):\n",
    "                wt_prime_chapeau.append(key + self.delta/pt_prime)\n",
    "            else:\n",
    "                wt_prime_chapeau.append(key)\n",
    "    \n",
    "        return np.array(wt_prime_chapeau)\n",
    "    \n",
    "    def is_asset_shortage(self,action,pt,wt):\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        for ind in big_s_minus:\n",
    "            if wt[ind+1]*pt < self.delta:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    #alright\n",
    "    def is_cash_shortage(self,action,pt,wt):\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        current_cash = wt[0]*pt\n",
    "        cash_after_selling = current_cash + (1-self.c_minus)*self.delta*len(big_s_minus) # must include transaction costs\n",
    "        cash_needed = (self.c_plus+1)*self.delta*len(big_s_plus) # must include transaction costs\n",
    "        \n",
    "        if(cash_after_selling < cash_needed):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def action_mapping(self,action,action_Q_values,pt,wt):\n",
    "        action = copy(action)\n",
    "        action_mapped = action\n",
    "        if self.is_asset_shortage(action,pt,wt):\n",
    "            action_mapped = self.rule2(action,action_Q_values,pt,wt)\n",
    "        elif self.is_cash_shortage(action,pt,wt):\n",
    "            action_mapped = self.rule1(action,action_Q_values,pt,wt)\n",
    "        \n",
    "        return action_mapped\n",
    "    \n",
    "    def rule1(self,action,action_Q_values,pt,wt):\n",
    "        MAXQ = np.NINF\n",
    "        action_selected = action\n",
    "        \n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        \n",
    "        for i in range(1,len(big_s_plus) + 1):\n",
    "            for c in combinations(big_s_plus, i):\n",
    "                new_action = copy(action)\n",
    "                for j in c:\n",
    "                    new_action[j] = 1\n",
    "\n",
    "                if not self.is_cash_shortage(new_action,pt,wt):\n",
    "                    new_action_index = self.find_action_index(new_action)\n",
    "                    new_action_Q_value = action_Q_values[new_action_index]\n",
    "                    \n",
    "                    if new_action_Q_value > MAXQ:\n",
    "                        MAXQ = new_action_Q_value\n",
    "                        action_selected = new_action\n",
    "        \n",
    "        return action_selected\n",
    "    \n",
    "    def rule2(self,action,action_Q_values,pt,wt):\n",
    "        for i in range(len(action)):\n",
    "            if wt[i+1]*pt < self.delta:\n",
    "                action[i] = 1\n",
    "        \n",
    "        if self.is_cash_shortage(action,pt,wt):\n",
    "            action = self.rule1(action,action_Q_values,pt,wt)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def F(self,pt,wt):\n",
    "        action_possible = []\n",
    "        for ind, action in enumerate(self.actions):\n",
    "            if not self.is_asset_shortage(action,pt,wt) and not self.is_cash_shortage(action,pt,wt):\n",
    "                action_possible.append(ind)\n",
    "\n",
    "        return np.array(action_possible)\n",
    "    \n",
    "    def step(self, action, simulation=False):\n",
    "        # Must set new portfolio with regards to action\n",
    "        # Must set new reward\n",
    "        if self.current_tick == len(self.big_pt) - 2:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            self.episode_ended = True\n",
    "        \n",
    "        # we are in state 0, best action between state 0 and state 1 has been predicted\n",
    "        # so we must get portfolio value and weights after state 0 evolution\n",
    "        # but before action has been taken into account\n",
    "        ktc = self.big_pt[self.current_tick,0,:,self.n-1] # nouveaux close prices des différents assets\n",
    "        pt_prime = self.pt * np.dot(self.wt,self.phi(ktc)) # nouvelle valeur du portfolio issue de l'action précédente\n",
    "        \n",
    "        wt_prime = (self.wt*self.phi(ktc)) / (np.dot(self.wt,self.phi(ktc))) # nouvelles proportions des assets du portfolio\n",
    "        \n",
    "        # On prend en compte la nouvelle action\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        \n",
    "        ct = (self.delta*(self.c_minus*len(big_s_minus) + self.c_plus*len(big_s_plus)))/pt_prime\n",
    "        if not simulation:\n",
    "            self.pt = pt_prime*(1 - ct)\n",
    "        else:\n",
    "            pt = pt_prime*(1 - ct)\n",
    "        \n",
    "        wt_prime_chapeau_1tillend = self.get_wt_prime_chapeau(wt_prime[1:],big_s_minus,big_s_plus,pt_prime)\n",
    "        wt_prime_chapeau_0 = wt_prime[0] + self.delta*((1-self.c_minus)*len(big_s_minus)-(1+self.c_plus)*len(big_s_plus))/pt_prime\n",
    "        wt_prime_chapeau = np.concatenate((np.array([wt_prime_chapeau_0]), wt_prime_chapeau_1tillend))\n",
    "        \n",
    "        # now we evolve to state one, to get reward of this action\n",
    "        if not simulation:\n",
    "            self.wt = wt_prime_chapeau / (np.dot(wt_prime_chapeau, np.ones(len(wt_prime_chapeau))))\n",
    "            self.current_tick += 1\n",
    "            k_t_plus_one_c = self.big_pt[self.current_tick,0,:,self.n-1]\n",
    "        else:\n",
    "            wt = wt_prime_chapeau / (np.dot(wt_prime_chapeau, np.ones(len(wt_prime_chapeau))))\n",
    "            current_tick = self.current_tick + 1\n",
    "            k_t_plus_one_c = self.big_pt[current_tick,0,:,self.n-1]\n",
    "        \n",
    "        big_p_s_t_plus_one = pt_prime*np.dot(wt_prime, self.phi(k_t_plus_one_c))\n",
    "        \n",
    "        if not simulation:\n",
    "            p_t_plus_one_prime = self.pt * np.dot(self.wt,self.phi(k_t_plus_one_c))\n",
    "            reward = (p_t_plus_one_prime - big_p_s_t_plus_one)/big_p_s_t_plus_one\n",
    "            wt_plus_one_prime = (self.wt*self.phi(k_t_plus_one_c)) / (np.dot(self.wt,self.phi(k_t_plus_one_c)))\n",
    "        else:\n",
    "            p_t_plus_one_prime = pt * np.dot(wt,self.phi(k_t_plus_one_c))\n",
    "            reward = (p_t_plus_one_prime - big_p_s_t_plus_one)/big_p_s_t_plus_one\n",
    "            wt_plus_one_prime = (wt*self.phi(k_t_plus_one_c)) / (np.dot(wt,self.phi(k_t_plus_one_c)))\n",
    "        \n",
    "        if not simulation:\n",
    "            return {'big_xt':np.array(self.small_pt[self.current_tick]), 'wt_prime':wt_plus_one_prime}, reward, self.episode_ended\n",
    "        else:\n",
    "            return {'big_xt':np.array(self.small_pt[current_tick]), 'wt_prime':wt_plus_one_prime}, reward, self.episode_ended, p_t_plus_one_prime, wt_plus_one_prime\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1741, 5, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"SPY\", \"IWD\", \"IWC\"]\n",
    "train_dates = [\"2010-01-01\", \"2016-12-30\"]\n",
    "n = 20\n",
    "env = PortfolioEnv(train_dates, datasets, n)\n",
    "datas_ = env.big_pt\n",
    "datas = datas_.swapaxes(2,3).swapaxes(1,3)\n",
    "final_datas = []\n",
    "for d in datas:\n",
    "    final_datas.append(d[0])\n",
    "    final_datas.append(d[1])\n",
    "    final_datas.append(d[2])\n",
    "final_datas = np.array(final_datas)\n",
    "mm_scaler = MinMaxScaler()\n",
    "datas_scaled = np.array([mm_scaler.fit_transform(d) for d in final_datas]) # MinMaxScaler par ligne ou par colonne ?\n",
    "X_train = datas_scaled[:4500]\n",
    "X_valid = datas_scaled[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 20, 128)           68608     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 20)                11920     \n",
      "=================================================================\n",
      "Total params: 80,528\n",
      "Trainable params: 80,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "repeat_vector_2 (RepeatVecto (None, 20, 20)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 20, 128)           76288     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 5)             645       \n",
      "=================================================================\n",
      "Total params: 76,933\n",
      "Trainable params: 76,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_6 (Sequential)    (None, 20)                80528     \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 20, 5)             76933     \n",
      "=================================================================\n",
      "Total params: 157,461\n",
      "Trainable params: 157,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "141/141 [==============================] - 2s 14ms/step - loss: 0.6506 - val_loss: 0.6417\n",
      "Epoch 2/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6376 - val_loss: 0.6371\n",
      "Epoch 3/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6339 - val_loss: 0.6331\n",
      "Epoch 4/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6305 - val_loss: 0.6300\n",
      "Epoch 5/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6277 - val_loss: 0.6280\n",
      "Epoch 6/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6254 - val_loss: 0.6259\n",
      "Epoch 7/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6234 - val_loss: 0.6237\n",
      "Epoch 8/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6219 - val_loss: 0.6218\n",
      "Epoch 9/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6192 - val_loss: 0.6194\n",
      "Epoch 10/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6173 - val_loss: 0.6179\n",
      "Epoch 11/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6147 - val_loss: 0.6159\n",
      "Epoch 12/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6126 - val_loss: 0.6146\n",
      "Epoch 13/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6110 - val_loss: 0.6140\n",
      "Epoch 14/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6098 - val_loss: 0.6145\n",
      "Epoch 15/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6086 - val_loss: 0.6127\n",
      "Epoch 16/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6073 - val_loss: 0.6116\n",
      "Epoch 17/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6053 - val_loss: 0.6092\n",
      "Epoch 18/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6033 - val_loss: 0.6095\n",
      "Epoch 19/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6021 - val_loss: 0.6085\n",
      "Epoch 20/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6009 - val_loss: 0.6079\n",
      "Epoch 21/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5990 - val_loss: 0.6076\n",
      "Epoch 22/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5975 - val_loss: 0.6076\n",
      "Epoch 23/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5961 - val_loss: 0.6051\n",
      "Epoch 24/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5939 - val_loss: 0.6035\n",
      "Epoch 25/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5917 - val_loss: 0.6021\n",
      "Epoch 26/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5905 - val_loss: 0.6028\n",
      "Epoch 27/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5892 - val_loss: 0.6022\n",
      "Epoch 28/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5886 - val_loss: 0.6012\n",
      "Epoch 29/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5866 - val_loss: 0.6004\n",
      "Epoch 30/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5853 - val_loss: 0.6008\n",
      "Epoch 31/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5848 - val_loss: 0.5995\n",
      "Epoch 32/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5836 - val_loss: 0.6006\n",
      "Epoch 33/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5822 - val_loss: 0.5994\n",
      "Epoch 34/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5813 - val_loss: 0.5998\n",
      "Epoch 35/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5801 - val_loss: 0.5984\n",
      "Epoch 36/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5798 - val_loss: 0.5982\n",
      "Epoch 37/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5784 - val_loss: 0.5994\n",
      "Epoch 38/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5773 - val_loss: 0.5963\n",
      "Epoch 39/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5764 - val_loss: 0.5969\n",
      "Epoch 40/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5750 - val_loss: 0.5975\n",
      "Epoch 41/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5743 - val_loss: 0.5955\n",
      "Epoch 42/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5738 - val_loss: 0.5967\n",
      "Epoch 43/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5723 - val_loss: 0.5942\n",
      "Epoch 44/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5714 - val_loss: 0.5965\n",
      "Epoch 45/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5707 - val_loss: 0.5957\n",
      "Epoch 46/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5706 - val_loss: 0.5943\n",
      "Epoch 47/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5691 - val_loss: 0.5942\n",
      "Epoch 48/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5686 - val_loss: 0.5942\n",
      "Epoch 49/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5676 - val_loss: 0.5943\n",
      "Epoch 50/50\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.5667 - val_loss: 0.5965\n"
     ]
    }
   ],
   "source": [
    "recurrent_encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units=128, input_shape=[20, 5], return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=20),\n",
    "])\n",
    "recurrent_decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.RepeatVector(20, input_shape=[20]),\n",
    "    tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation=\"sigmoid\"))\n",
    "])\n",
    "recurrent_ae = tf.keras.models.Sequential([recurrent_encoder, recurrent_decoder])\n",
    "recurrent_ae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "recurrent_encoder.summary()\n",
    "recurrent_decoder.summary()\n",
    "recurrent_ae.summary()\n",
    "\n",
    "history = recurrent_ae.fit(X_train, X_train, epochs=50, validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230, 5, 3, 20)\n",
      "(231, 5, 3, 20)\n",
      "(229, 5, 3, 20)\n",
      "(230, 5, 3, 20)\n",
      "(230, 5, 3, 20)\n",
      "(230, 5, 3, 20)\n",
      "(231, 5, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "def create_envs(dates, datasets, n):\n",
    "    return np.array([PortfolioEnv(d, datasets, n, encoder=recurrent_encoder) for d in dates])\n",
    "datasets = [\"SPY\", \"IWD\", \"IWC\"]\n",
    "train_dates = [\n",
    "    [\"2010-01-01\", \"2010-12-30\"],\n",
    "    [\"2011-01-01\", \"2011-12-30\"],\n",
    "    [\"2012-01-01\", \"2012-12-30\"],\n",
    "    [\"2013-01-01\", \"2013-12-30\"],\n",
    "    [\"2014-01-01\", \"2014-12-30\"],\n",
    "    [\"2015-01-01\", \"2015-12-30\"],\n",
    "    [\"2016-01-01\", \"2016-12-30\"],\n",
    "]\n",
    "train_envs = create_envs(train_dates, datasets, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, 5, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "test_dates = [\n",
    "    [\"2017-01-01\", \"2017-12-30\"],\n",
    "]\n",
    "test_envs = create_envs(test_dates, datasets, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e2f02d03d54039a926aadb1ae514a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727c9d35f101435d874e89842ffb9d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-344ae36344d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mpreprocessed_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msimulation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimulations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfield_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mnext_Q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_next_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Q-values predicted for all next_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mmax_next_Q_values_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_Q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for each next_states find index of max Q-value predicted by target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@tf.function\n",
    "def train_step(preprocessed_states, mask, target_Q_values):\n",
    "    with tf.device('gpu:0'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = model(preprocessed_states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "beta = .3\n",
    "epochs = 500 \n",
    "\n",
    "r = np.random.rand(epochs)\n",
    "N=len(train_envs)\n",
    "gen_trunc=(N-1-np.floor(np.log(1-r*(1-(1-beta)**N))/np.log(1-beta))).astype(int)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "K = tf.keras.backend\n",
    "\n",
    "input_states = tf.keras.layers.Input(shape=[64])\n",
    "hidden1 = tf.keras.layers.Dense(64, activation=\"relu\")(input_states)\n",
    "hidden2 = tf.keras.layers.Dense(32, activation=\"relu\")(hidden1)\n",
    "output = tf.keras.layers.Dense(train_envs[0].action_shape)(hidden2)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_states], outputs=[output])\n",
    "\n",
    "target = tf.keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n",
    "\n",
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "epsilon = 1\n",
    "batch_size = 16\n",
    "discount_rate = .9\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-7)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "steps = 0\n",
    "episode_counter = 1\n",
    "\n",
    "train_CR = []\n",
    "previous_CR = 0\n",
    "\n",
    "test_CR = []\n",
    "test_previous_CR = 0\n",
    "\n",
    "for ind in gen_trunc:\n",
    "    env = train_envs[ind]\n",
    "    state = env.reset()\n",
    "        \n",
    "    pbar = tqdm(total=len(env.big_pt)-1)\n",
    "    \n",
    "    epsilon = max(1 - episode_counter / 400, 0.01)\n",
    "    \n",
    "    while 1: # will break when end of env\n",
    "        pbar.set_description(f'e.{episode_counter}/{len(gen_trunc)}|tr.p.CR:{previous_CR}|te.p.CR:{test_previous_CR}')\n",
    "        #epsilon-greedy policy to select an action\n",
    "        action = None\n",
    "        state_preprocessed = np.concatenate((state['wt_prime'], state['big_xt']))\n",
    "        \n",
    "        k_t_plus_one_c = env.big_pt[env.current_tick,0,:,env.n-1]\n",
    "        p_t_plus_one_prime = env.pt * np.dot(env.wt,env.phi(k_t_plus_one_c))\n",
    "        wt_plus_one_prime = (env.wt*env.phi(k_t_plus_one_c)) / (np.dot(env.wt,env.phi(k_t_plus_one_c)))\n",
    "        \n",
    "        if np.random.rand() < epsilon: # goal is to take uniformly random action among possible actions of this env\n",
    "            possible_actions = env.F(p_t_plus_one_prime, wt_plus_one_prime) # when we sell/buy it's new prices, not protfolio\n",
    "            # value when action A has been taken, so when action A has been taken and then new state has come\n",
    "\n",
    "            random_index = np.random.randint(len(possible_actions))\n",
    "            action = env.actions[possible_actions[random_index]]\n",
    "        else: # here goal is to make model predict best action, but if action isn't feasable for env, map it\n",
    "            Q_values = model.predict(state_preprocessed[np.newaxis])\n",
    "            action = env.action_mapping(env.actions[np.argmax(Q_values[0])], Q_values[0], p_t_plus_one_prime, wt_plus_one_prime)\n",
    "        \n",
    "        # simulate all feasible actions for current state\n",
    "        possible_actions = env.F(p_t_plus_one_prime, wt_plus_one_prime) # here problem too\n",
    "        simulations = []\n",
    "        \n",
    "        for simulated_action in possible_actions:\n",
    "            next_state_simulated, next_reward, done, next_pt, next_wt = env.step(env.actions[simulated_action], simulation=True)\n",
    "\n",
    "            next_state_simulated_preprocessed = np.concatenate((next_state_simulated['wt_prime'], next_state_simulated['big_xt']))\n",
    "            simulations.append((state_preprocessed, simulated_action, next_reward, next_state_simulated_preprocessed, done, next_pt, next_wt))\n",
    "        replay_memory.append(np.array(simulations))\n",
    "        next_state, reward, episode_ended = env.step(action)\n",
    "        # do some stuff with it\n",
    "        \n",
    "        # add some time for buffer to populate\n",
    "        \n",
    "        if steps >= 50:\n",
    "            # start batch training (room to improvment, why don't use Prioritized Experience Replay ?)\n",
    "            # select a uniformly random batch\n",
    "            indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "            batch = [replay_memory[index] for index in indices]\n",
    "            for simulations in batch:\n",
    "                #start = time.time()\n",
    "                preprocessed_states, actions, rewards, preprocessed_next_states, dones, next_pts, next_wts = [np.array([simulation[field_index] for simulation in simulations]) for field_index in range(7)]\n",
    "                \n",
    "                next_Q_values = target.predict(preprocessed_next_states) # Q-values predicted for all next_states\n",
    "                max_next_Q_values_index = np.argmax(next_Q_values, axis=1) # for each next_states find index of max Q-value predicted by target\n",
    "                \n",
    "                # okay, here, in max_next_Q_values, there are some actions that may not be feasible and that we need to map\n",
    "                # but we have to use is_asset_shortage, is_cash_shortage and action_mapping with state of simulation\n",
    "                max_next_Q_values_actions = np.array([\n",
    "                    env.actions[action_simulated] if not env.is_asset_shortage(env.actions[action_simulated], next_pts[ind], next_wts[ind]) and not env.is_cash_shortage(env.actions[action_simulated], next_pts[ind], next_wts[ind])\n",
    "                    else env.action_mapping(env.actions[action_simulated], next_Q_values[ind], next_pts[ind], next_wts[ind])\n",
    "                    for ind, action_simulated in enumerate(max_next_Q_values_index)\n",
    "                ])\n",
    "                \n",
    "                # okay, we have the feasible actions selected by the target network, now we need to find their indexes to communicate with the network\n",
    "                max_next_Q_values_indexes = [env.find_action_index(action_simulated) for action_simulated in max_next_Q_values_actions]\n",
    "                # now we have to find the Q-values predicted by target network of these corrected actions\n",
    "                max_next_Q_values = [next_Q_values[ind][best_action_simulated] for ind, best_action_simulated in enumerate(max_next_Q_values_indexes)]\n",
    "\n",
    "                # and now, we can proprely calculate the target Q-value\n",
    "                target_Q_values = (rewards +\n",
    "                                   (1 - dones) * discount_rate * max_next_Q_values) # here problem with dones /!\\\n",
    "                target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "                mask = tf.one_hot(actions, env.action_shape)\n",
    "                \n",
    "                train_step(preprocessed_states, mask, target_Q_values)\n",
    "                # and we're done, amazing !!\n",
    "                #print('updates ::: {:.5f} ms / step'.format((time.time() - start) * 1000))\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "        steps += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if episode_ended:\n",
    "            break;\n",
    "        \n",
    "    episode_counter += 1\n",
    "    \n",
    "    previous_CR = round((env.pt-env.initial_pt)/env.initial_pt, 2)\n",
    "    train_CR.append(previous_CR)\n",
    "    pbar.close()\n",
    "    \n",
    "    for test_env in test_envs:\n",
    "        test_state = test_env.reset()\n",
    "        # state 0 return\n",
    "        # but current_tick set to one\n",
    "        test_episode_ended = False\n",
    "        while not test_episode_ended:\n",
    "            # state 0\n",
    "            test_state_preprocessed = np.concatenate((test_state['wt_prime'], test_state['big_xt']))\n",
    "            # Q-values of action from state 0 predicted\n",
    "            # algorithm only knows state 0 and want to predict best action and then state one happens\n",
    "            test_Q_values = model.predict(test_state_preprocessed[np.newaxis])\n",
    "            \n",
    "            # we want to get total value of portfolio and new portfolio weights regarding state 0 evolution\n",
    "            # so we enter the algorithm with an already predefined portfolio distribution\n",
    "            # then state 0 happens\n",
    "            # so here current_tick should 0, not one,\n",
    "            # to calculate total value of portfolio and new portfolio weights regarding state 0 evolution\n",
    "            test_k_t_plus_one_c = test_env.big_pt[test_env.current_tick,0,:,test_env.n-1]\n",
    "            test_p_t_plus_one_prime = test_env.pt * np.dot(test_env.wt,test_env.phi(test_k_t_plus_one_c))\n",
    "            test_wt_plus_one_prime = (test_env.wt*test_env.phi(test_k_t_plus_one_c)) / (np.dot(test_env.wt,test_env.phi(test_k_t_plus_one_c)))\n",
    "            \n",
    "            test_action = test_env.action_mapping(test_env.actions[np.argmax(test_Q_values[0])], test_Q_values[0], test_p_t_plus_one_prime, test_wt_plus_one_prime)\n",
    "            \n",
    "            # okay here, we have selected best action predicted after state 0 evolution\n",
    "            # and we want to evolve to state one\n",
    "            test_state, test_reward, test_episode_ended = test_env.step(test_action)\n",
    "        \n",
    "        test_previous_CR = round((test_env.pt-test_env.initial_pt)/test_env.initial_pt, 3)\n",
    "        test_CR.append(test_previous_CR)\n",
    "        \n",
    "    # update of target network\n",
    "    target.set_weights(model.get_weights()) \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "import pickle as pkl\n",
    "pkl.dump(train_CR, open('train_CR.pkl', 'wb'))\n",
    "pkl.dump(test_CR, open('test_CR.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
