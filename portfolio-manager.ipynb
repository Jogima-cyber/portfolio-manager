{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import copy\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv():\n",
    "    def __init__(self, dates, datasets, n):\n",
    "        self.dates = dates\n",
    "        self.datasets = datasets\n",
    "        self.n = n\n",
    "        self.process_big_pt()\n",
    "        \n",
    "        self.process_small_pt()\n",
    "        \n",
    "        self.initial_pt = 1000000 # 1 000 000\n",
    "        self.c_minus = 0.0025 # 0.25%\n",
    "        self.c_plus = 0.0025 # 0.25%\n",
    "        self.delta = 10000 # 10 000\n",
    "                \n",
    "        self.process_actions()\n",
    "        self.action_shape = self.actions.shape[0]\n",
    "        self._episode_ended = False\n",
    "    \n",
    "    def reset(self):\n",
    "        # initialisation of portfolio\n",
    "        self.pt = self.initial_pt # 1 000 000\n",
    "        self.wt = [0.25,0.25,0.25,0.25] # wt before state 0\n",
    "        \n",
    "        self.current_tick = 0 # after checking, current_tick should be set to 0\n",
    "        self.episode_ended = False\n",
    "        \n",
    "        ktc = self.big_pt[self.current_tick,0,:,self.n-1] # nouveaux close prices des diff√©rents assets\n",
    "        wt_prime = (self.wt*self.phi(ktc)) / (np.dot(self.wt,self.phi(ktc))) # nouvelles proportions des assets du portfolio\n",
    "        \n",
    "        # after checking, we should return state 0\n",
    "        return {'big_xt':np.array(self.small_pt[self.current_tick]), 'wt_prime':wt_prime}\n",
    "    \n",
    "    def process_actions(self):\n",
    "        asset_number = 3\n",
    "        action_number = 3\n",
    "\n",
    "        seq = np.arange(asset_number)\n",
    "        actions = []\n",
    "\n",
    "        for c in product(seq, repeat=action_number):\n",
    "            actions.append(c)\n",
    "\n",
    "        self.actions = np.array(actions)\n",
    "    \n",
    "    def find_action_index(self,action):\n",
    "        for ind, a in enumerate(self.actions):\n",
    "            if np.array_equal(a, action):\n",
    "                return ind;\n",
    "    \n",
    "    def process_big_pt(self):\n",
    "        datasets = self.datasets\n",
    "        date_start = self.dates[0]\n",
    "        date_end = self.dates[1]\n",
    "\n",
    "        dfs = []\n",
    "        for d in datasets:\n",
    "            ticker = yf.Ticker(d)\n",
    "            # get historical market data\n",
    "            df_ = ticker.history(start=date_start, end=date_end, interval=\"1d\")    \n",
    "            df_.rename(mapper={\n",
    "                \"Close\": d+\"_close\",\n",
    "                \"Open\": d+\"_open\",\n",
    "                \"High\": d+\"_high\",\n",
    "                \"Low\": d+\"_low\",\n",
    "                \"Volume\": d+\"_volume\"\n",
    "            }, inplace=True, axis=1)\n",
    "            if \"Dividends\" in df_.columns:\n",
    "                df_.drop(axis=1,labels=[\"Dividends\", \"Stock Splits\"],inplace=True)\n",
    "            dfs.append(df_)\n",
    "\n",
    "        final_df = pd.concat(dfs, axis=1)\n",
    "        final_df.dropna(inplace=True)\n",
    "        \n",
    "        self.final_df = final_df\n",
    "        \n",
    "        final_df = self.final_df\n",
    "        n = self.n\n",
    "        Pc = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_prev_close = final_df[d+\"_close\"].shift().values\n",
    "            Kc = (asset_close - asset_prev_close) / asset_prev_close\n",
    "            Kc = Kc[1:]\n",
    "            Pc_ = [Kc[i:i+n] for i in range(len(asset_close)-n)] # Kc[0:20], Kc[1:21], Kc[2:22]\n",
    "            Pc.append(Pc_)\n",
    "        Pc = np.array(Pc)\n",
    "        Po = []\n",
    "        for d in datasets:\n",
    "            asset_prev_close = final_df[d+\"_close\"].shift().values\n",
    "            asset_open = final_df[d+\"_open\"].values\n",
    "            Ko = (asset_open - asset_prev_close) / asset_prev_close\n",
    "            Ko = Ko[1:]\n",
    "            Po_ = [Ko[i:i+n] for i in range(len(asset_open)-n)]\n",
    "            Po.append(Po_)\n",
    "        Po = np.array(Po)\n",
    "        Pl = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_low = final_df[d+\"_low\"].values\n",
    "            Kl = (asset_close - asset_low) / asset_low\n",
    "            Kl = Kl[1:]\n",
    "            Pl_ = [Kl[i:i+n] for i in range(len(asset_low)-n)]\n",
    "            Pl.append(Pl_)\n",
    "        Pl = np.array(Pl)\n",
    "        Ph = []\n",
    "        for d in datasets:\n",
    "            asset_close = final_df[d+\"_close\"].values\n",
    "            asset_high = final_df[d+\"_high\"].values\n",
    "            Kh = (asset_close - asset_high) / asset_high\n",
    "            Kh = Kh[1:]\n",
    "            Ph_ = [Kh[i:i+n] for i in range(len(asset_high)-n)]\n",
    "            Ph.append(Ph_)\n",
    "        Ph = np.array(Ph)\n",
    "        Pv = []\n",
    "        for d in datasets:\n",
    "            asset_prev_volume = final_df[d+\"_volume\"].shift().values\n",
    "            asset_volume = final_df[d+\"_volume\"].values\n",
    "            Kv = (asset_volume - asset_prev_volume) / asset_prev_volume\n",
    "            Kv = Kv[1:]\n",
    "            Pv_ = [Kv[i:i+n] for i in range(len(asset_high)-n)]\n",
    "            Pv.append(Pv_)\n",
    "        Pv = np.array(Pv)\n",
    "        Pt_star = np.array([Pc, Po, Pl, Ph, Pv])\n",
    "\n",
    "        self.big_pt = Pt_star.swapaxes(0,2).swapaxes(1,2)\n",
    "        print(self.big_pt.shape)\n",
    "        \n",
    "    def process_small_pt(self):\n",
    "        small_pt = []\n",
    "        \n",
    "        for big_xt in self.big_pt:\n",
    "            big_xt = big_xt.swapaxes(0,1).swapaxes(1,2)\n",
    "            big_xt_scaled = np.array([mm_scaler.fit_transform(a) for a in big_xt])\n",
    "\n",
    "            small_pt.append(big_xt_scaled)\n",
    "        \n",
    "        self.small_pt = np.array(small_pt)\n",
    "        print(self.small_pt.shape)\n",
    "    \n",
    "    def phi(self,v):\n",
    "        return np.insert(v, 0, [0]) + np.ones(len(v) + 1)\n",
    "    \n",
    "    def get_wt_prime_chapeau(self,wt_prime,big_s_minus,big_s_plus,pt_prime):\n",
    "        wt_prime_chapeau = []\n",
    "        for ind, key in enumerate(wt_prime):\n",
    "            if(ind in big_s_minus):\n",
    "                wt_prime_chapeau.append(key - self.delta/pt_prime)\n",
    "            elif(ind in big_s_plus):\n",
    "                wt_prime_chapeau.append(key + self.delta/pt_prime)\n",
    "            else:\n",
    "                wt_prime_chapeau.append(key)\n",
    "    \n",
    "        return np.array(wt_prime_chapeau)\n",
    "    \n",
    "    def is_asset_shortage(self,action,pt,wt):\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        for ind in big_s_minus:\n",
    "            if wt[ind+1]*pt < self.delta:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    #alright\n",
    "    def is_cash_shortage(self,action,pt,wt):\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        current_cash = wt[0]*pt\n",
    "        cash_after_selling = current_cash + (1-self.c_minus)*self.delta*len(big_s_minus) # must include transaction costs\n",
    "        cash_needed = (self.c_plus+1)*self.delta*len(big_s_plus) # must include transaction costs\n",
    "        \n",
    "        if(cash_after_selling < cash_needed):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def action_mapping(self,action,action_Q_values,pt,wt):\n",
    "        action = copy(action)\n",
    "        action_mapped = action\n",
    "        if self.is_asset_shortage(action,pt,wt):\n",
    "            action_mapped = self.rule2(action,action_Q_values,pt,wt)\n",
    "        elif self.is_cash_shortage(action,pt,wt):\n",
    "            action_mapped = self.rule1(action,action_Q_values,pt,wt)\n",
    "        \n",
    "        return action_mapped\n",
    "    \n",
    "    def rule1(self,action,action_Q_values,pt,wt):\n",
    "        MAXQ = np.NINF\n",
    "        action_selected = action\n",
    "        \n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        \n",
    "        for i in range(1,len(big_s_plus) + 1):\n",
    "            for c in combinations(big_s_plus, i):\n",
    "                new_action = copy(action)\n",
    "                for j in c:\n",
    "                    new_action[j] = 1\n",
    "\n",
    "                if not self.is_cash_shortage(new_action,pt,wt):\n",
    "                    new_action_index = self.find_action_index(new_action)\n",
    "                    new_action_Q_value = action_Q_values[new_action_index]\n",
    "                    \n",
    "                    if new_action_Q_value > MAXQ:\n",
    "                        MAXQ = new_action_Q_value\n",
    "                        action_selected = new_action\n",
    "        \n",
    "        return action_selected\n",
    "    \n",
    "    def rule2(self,action,action_Q_values,pt,wt):\n",
    "        for i in range(len(action)):\n",
    "            if wt[i+1]*pt < self.delta:\n",
    "                action[i] = 1\n",
    "        \n",
    "        if self.is_cash_shortage(action,pt,wt):\n",
    "            action = self.rule1(action,action_Q_values,pt,wt)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def F(self,pt,wt):\n",
    "        action_possible = []\n",
    "        for ind, action in enumerate(self.actions):\n",
    "            if not self.is_asset_shortage(action,pt,wt) and not self.is_cash_shortage(action,pt,wt):\n",
    "                action_possible.append(ind)\n",
    "\n",
    "        return np.array(action_possible)\n",
    "    \n",
    "    def step(self, action, simulation=False):\n",
    "        # Must set new portfolio with regards to action\n",
    "        # Must set new reward\n",
    "        if self.current_tick == len(self.big_pt) - 2:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            self.episode_ended = True\n",
    "        \n",
    "        # we are in state 0, best action between state 0 and state 1 has been predicted\n",
    "        # so we must get portfolio value and weights after state 0 evolution\n",
    "        # but before action has been taken into account\n",
    "        ktc = self.big_pt[self.current_tick,0,:,self.n-1] # nouveaux close prices des diff√©rents assets\n",
    "        pt_prime = self.pt * np.dot(self.wt,self.phi(ktc)) # nouvelle valeur du portfolio issue de l'action pr√©c√©dente\n",
    "        wt_prime = (self.wt*self.phi(ktc)) / (np.dot(self.wt,self.phi(ktc))) # nouvelles proportions des assets du portfolio\n",
    "        \n",
    "        # On prend en compte la nouvelle action\n",
    "        big_s_minus = np.where(action==0)[0]\n",
    "        big_s_plus = np.where(action==2)[0]\n",
    "        \n",
    "        ct = (self.delta*(self.c_minus*len(big_s_minus) + self.c_plus*len(big_s_plus)))/pt_prime\n",
    "        if not simulation:\n",
    "            self.pt = pt_prime*(1 - ct)\n",
    "        else:\n",
    "            pt = pt_prime*(1 - ct)\n",
    "        \n",
    "        wt_prime_chapeau_1tillend = self.get_wt_prime_chapeau(wt_prime[1:],big_s_minus,big_s_plus,pt_prime)\n",
    "        wt_prime_chapeau_0 = wt_prime[0] + self.delta*((1-self.c_minus)*len(big_s_minus)-(1+self.c_plus)*len(big_s_plus))/pt_prime\n",
    "        wt_prime_chapeau = np.concatenate((np.array([wt_prime_chapeau_0]), wt_prime_chapeau_1tillend))\n",
    "        \n",
    "        # now we evolve to state one, to get reward of this action\n",
    "        if not simulation:\n",
    "            self.wt = wt_prime_chapeau / (np.dot(wt_prime_chapeau, np.ones(len(wt_prime_chapeau))))\n",
    "            self.current_tick += 1\n",
    "            k_t_plus_one_c = self.big_pt[self.current_tick,0,:,self.n-1]\n",
    "        else:\n",
    "            wt = wt_prime_chapeau / (np.dot(wt_prime_chapeau, np.ones(len(wt_prime_chapeau))))\n",
    "            current_tick = self.current_tick + 1\n",
    "            k_t_plus_one_c = self.big_pt[current_tick,0,:,self.n-1]\n",
    "        \n",
    "        big_p_s_t_plus_one = pt_prime*np.dot(wt_prime, self.phi(k_t_plus_one_c))\n",
    "        \n",
    "        if not simulation:\n",
    "            p_t_plus_one_prime = self.pt * np.dot(self.wt,self.phi(k_t_plus_one_c))\n",
    "            reward = (p_t_plus_one_prime - big_p_s_t_plus_one)/big_p_s_t_plus_one\n",
    "            wt_plus_one_prime = (self.wt*self.phi(k_t_plus_one_c)) / (np.dot(self.wt,self.phi(k_t_plus_one_c)))\n",
    "        else:\n",
    "            p_t_plus_one_prime = pt * np.dot(wt,self.phi(k_t_plus_one_c))\n",
    "            reward = (p_t_plus_one_prime - big_p_s_t_plus_one)/big_p_s_t_plus_one\n",
    "            wt_plus_one_prime = (wt*self.phi(k_t_plus_one_c)) / (np.dot(wt,self.phi(k_t_plus_one_c)))\n",
    "        \n",
    "        if not simulation:\n",
    "            return {'big_xt':np.array(self.small_pt[self.current_tick]), 'wt_prime':wt_plus_one_prime}, reward, self.episode_ended\n",
    "        else:\n",
    "            return {'big_xt':np.array(self.small_pt[current_tick]), 'wt_prime':wt_plus_one_prime}, reward, self.episode_ended, p_t_plus_one_prime, wt_plus_one_prime\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2748, 5, 3, 20)\n",
      "(2748, 3, 20, 5)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"SPY\", \"IWD\", \"IWC\"]\n",
    "train_dates = [\"2006-01-01\", \"2016-12-30\"] # let's train it on more datas to see what happens\n",
    "n = 20\n",
    "env = PortfolioEnv(train_dates, datasets, n)\n",
    "datas_ = env.big_pt\n",
    "datas = datas_.swapaxes(2,3).swapaxes(1,3)\n",
    "final_datas = []\n",
    "for d in datas:\n",
    "    final_datas.append(d[0])\n",
    "    final_datas.append(d[1])\n",
    "    final_datas.append(d[2])\n",
    "final_datas = np.array(final_datas)\n",
    "mm_scaler = MinMaxScaler()\n",
    "datas_scaled = np.array([mm_scaler.fit_transform(d) for d in final_datas]) # MinMaxScaler par ligne ou par colonne ?\n",
    "X_train = datas_scaled[:7200]\n",
    "X_valid = datas_scaled[7200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 20, 128)           68608     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 20)                11920     \n",
      "=================================================================\n",
      "Total params: 80,528\n",
      "Trainable params: 80,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "repeat_vector_1 (RepeatVecto (None, 20, 20)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 20, 128)           76288     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 5)             645       \n",
      "=================================================================\n",
      "Total params: 76,933\n",
      "Trainable params: 76,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    (None, 20)                80528     \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 20, 5)             76933     \n",
      "=================================================================\n",
      "Total params: 157,461\n",
      "Trainable params: 157,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "225/225 [==============================] - 2s 10ms/step - loss: 0.6470 - val_loss: 0.6379\n",
      "Epoch 2/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.6366 - val_loss: 0.6304\n",
      "Epoch 3/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6314 - val_loss: 0.6259\n",
      "Epoch 4/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6276 - val_loss: 0.6223\n",
      "Epoch 5/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6244 - val_loss: 0.6184\n",
      "Epoch 6/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6207 - val_loss: 0.6151\n",
      "Epoch 7/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6172 - val_loss: 0.6121\n",
      "Epoch 8/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6147 - val_loss: 0.6103\n",
      "Epoch 9/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6111 - val_loss: 0.6060\n",
      "Epoch 10/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6085 - val_loss: 0.6038\n",
      "Epoch 11/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6069 - val_loss: 0.6036\n",
      "Epoch 12/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6047 - val_loss: 0.6003\n",
      "Epoch 13/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.6014 - val_loss: 0.5989\n",
      "Epoch 14/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5999 - val_loss: 0.5979\n",
      "Epoch 15/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5974 - val_loss: 0.5954\n",
      "Epoch 16/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5946 - val_loss: 0.5940\n",
      "Epoch 17/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5928 - val_loss: 0.5918\n",
      "Epoch 18/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5906 - val_loss: 0.5900\n",
      "Epoch 19/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5884 - val_loss: 0.5887\n",
      "Epoch 20/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5868 - val_loss: 0.5878\n",
      "Epoch 21/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5848 - val_loss: 0.5853\n",
      "Epoch 22/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5832 - val_loss: 0.5851\n",
      "Epoch 23/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5818 - val_loss: 0.5840\n",
      "Epoch 24/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5803 - val_loss: 0.5830\n",
      "Epoch 25/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5798 - val_loss: 0.5816\n",
      "Epoch 26/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5776 - val_loss: 0.5810\n",
      "Epoch 27/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5772 - val_loss: 0.5790\n",
      "Epoch 28/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5760 - val_loss: 0.5783\n",
      "Epoch 29/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5751 - val_loss: 0.5776\n",
      "Epoch 30/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5745 - val_loss: 0.5778\n",
      "Epoch 31/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5733 - val_loss: 0.5772\n",
      "Epoch 32/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5726 - val_loss: 0.5779\n",
      "Epoch 33/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5720 - val_loss: 0.5769\n",
      "Epoch 34/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5713 - val_loss: 0.5764\n",
      "Epoch 35/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5706 - val_loss: 0.5760\n",
      "Epoch 36/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5699 - val_loss: 0.5749\n",
      "Epoch 37/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5694 - val_loss: 0.5764\n",
      "Epoch 38/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5689 - val_loss: 0.5765\n",
      "Epoch 39/50\n",
      "225/225 [==============================] - 1s 7ms/step - loss: 0.5682 - val_loss: 0.5742\n",
      "Epoch 40/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5671 - val_loss: 0.5739\n",
      "Epoch 41/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5665 - val_loss: 0.5735\n",
      "Epoch 42/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5658 - val_loss: 0.5733\n",
      "Epoch 43/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5656 - val_loss: 0.5734\n",
      "Epoch 44/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5651 - val_loss: 0.5736\n",
      "Epoch 45/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5646 - val_loss: 0.5731\n",
      "Epoch 46/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5638 - val_loss: 0.5742\n",
      "Epoch 47/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5636 - val_loss: 0.5729\n",
      "Epoch 48/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5629 - val_loss: 0.5736\n",
      "Epoch 49/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5624 - val_loss: 0.5730\n",
      "Epoch 50/50\n",
      "225/225 [==============================] - 2s 7ms/step - loss: 0.5620 - val_loss: 0.5725\n"
     ]
    }
   ],
   "source": [
    "recurrent_encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units=128, input_shape=[20, 5], return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=20),\n",
    "])\n",
    "recurrent_decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.RepeatVector(20, input_shape=[20]),\n",
    "    tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation=\"sigmoid\"))\n",
    "])\n",
    "recurrent_ae = tf.keras.models.Sequential([recurrent_encoder, recurrent_decoder])\n",
    "recurrent_ae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "recurrent_encoder.summary()\n",
    "recurrent_decoder.summary()\n",
    "recurrent_ae.summary()\n",
    "\n",
    "history = recurrent_ae.fit(X_train, X_train, epochs=50, validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230, 5, 3, 20)\n",
      "(230, 3, 20, 5)\n",
      "(231, 5, 3, 20)\n",
      "(231, 3, 20, 5)\n",
      "(229, 5, 3, 20)\n",
      "(229, 3, 20, 5)\n",
      "(230, 5, 3, 20)\n",
      "(230, 3, 20, 5)\n",
      "(230, 5, 3, 20)\n",
      "(230, 3, 20, 5)\n",
      "(230, 5, 3, 20)\n",
      "(230, 3, 20, 5)\n",
      "(231, 5, 3, 20)\n",
      "(231, 3, 20, 5)\n"
     ]
    }
   ],
   "source": [
    "def create_envs(dates, datasets, n):\n",
    "    return np.array([PortfolioEnv(d, datasets, n) for d in dates])\n",
    "datasets = [\"SPY\", \"IWD\", \"IWC\"]\n",
    "train_dates = [\n",
    "    [\"2010-01-01\", \"2010-12-30\"],\n",
    "    [\"2011-01-01\", \"2011-12-30\"],\n",
    "    [\"2012-01-01\", \"2012-12-30\"],\n",
    "    [\"2013-01-01\", \"2013-12-30\"],\n",
    "    [\"2014-01-01\", \"2014-12-30\"],\n",
    "    [\"2015-01-01\", \"2015-12-30\"],\n",
    "    [\"2016-01-01\", \"2016-12-30\"],\n",
    "]\n",
    "train_envs = create_envs(train_dates, datasets, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, 5, 3, 20)\n",
      "(231, 3, 20, 5)\n"
     ]
    }
   ],
   "source": [
    "test_dates = [\n",
    "    [\"2017-01-01\", \"2017-12-30\"],\n",
    "]\n",
    "test_envs = create_envs(test_dates, datasets, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 20, 5)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 20, 5)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 20, 5)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 20)           80528       input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60)           0           sequential_3[56][0]              \n",
      "                                                                 sequential_3[57][0]              \n",
      "                                                                 sequential_3[58][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           input_1[0][0]                    \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           4160        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 27)           891         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 87,659\n",
      "Trainable params: 87,659\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2617c7dfe7407286fa94ef053ca45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d15ee40b73b4a758e6535c35de6b293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21995f69a8647dca747972ba5c0134f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1af5c6f699c4adf9e88ab281d0704dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd44f42aab14271bbcac4c984bdbc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74015f5bf9e64de8b18654d6c1d75a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ae4c3a4e5d484c9b31e0b7e3ce2deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=228.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee03dacf7f944e4aa220ab89f8ca089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc28b90f1df54652b17410221281782a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6786144e4a44d919bfa173ee54259e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec31fb7a6114012973ca1b8b77d80ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f2cd4e129846b3813abae34ce452e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f1324b54d44f7bbcc1c4117a71a3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d657fda02f314d1e8b8038c0ef3ec7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@tf.function\n",
    "def train_step(preprocessed_states, wts_prime, mask, target_Q_values):\n",
    "    with tf.device('gpu:0'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = model((wts_prime, preprocessed_states[:,0], preprocessed_states[:,1], preprocessed_states[:,2]))\n",
    "            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "            # Q_values ne concerne pas les actions pas concern√©es, de m√™me que target_Q_values\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "beta = .3\n",
    "epochs = 500 \n",
    "\n",
    "r = np.random.rand(epochs)\n",
    "N=len(train_envs)\n",
    "gen_trunc=(N-1-np.floor(np.log(1-r*(1-(1-beta)**N))/np.log(1-beta))).astype(int)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "K = tf.keras.backend\n",
    "\n",
    "wt_prime_inpt = tf.keras.layers.Input(shape=[4])\n",
    "inputs = [wt_prime_inpt]\n",
    "encoders = []\n",
    "for t in datasets:\n",
    "    inpt = tf.keras.layers.Input(shape=[20, 5])\n",
    "    inputs.append(inpt)\n",
    "    encoder = recurrent_encoder(inpt)\n",
    "    encoders.append(encoder)\n",
    "\n",
    "enc_concat = tf.keras.layers.Concatenate(axis=1)(encoders)\n",
    "wt_prime_concat_with_enc = tf.keras.layers.Concatenate(axis=1)([wt_prime_inpt, enc_concat])\n",
    "\n",
    "hidden1 = tf.keras.layers.Dense(64, activation=\"relu\")(wt_prime_concat_with_enc)\n",
    "hidden2 = tf.keras.layers.Dense(32, activation=\"relu\")(hidden1)\n",
    "output = tf.keras.layers.Dense(train_envs[0].action_shape)(hidden2)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[inputs], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "target = tf.keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n",
    "\n",
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "epsilon = 1\n",
    "batch_size = 32\n",
    "discount_rate = .9\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-7)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "steps = 0\n",
    "episode_counter = 1\n",
    "\n",
    "train_CR = [[] for env in train_envs]\n",
    "previous_CR = 0\n",
    "\n",
    "test_CR = []\n",
    "test_previous_CR = 0\n",
    "\n",
    "for ind in gen_trunc:\n",
    "    env = train_envs[ind]\n",
    "    state = env.reset()\n",
    "        \n",
    "    pbar = tqdm(total=len(env.big_pt)-1)\n",
    "        \n",
    "    while 1: # will break when end of env\n",
    "        pbar.set_description(f'e.{episode_counter}/{len(gen_trunc)}|tr.p.CR:{previous_CR}|te.p.CR:{test_previous_CR}')\n",
    "        #epsilon-greedy policy to select an action\n",
    "        action = None        \n",
    "\n",
    "        k_t_plus_one_c = env.big_pt[env.current_tick,0,:,env.n-1]\n",
    "        p_t_plus_one_prime = env.pt * np.dot(env.wt,env.phi(k_t_plus_one_c))\n",
    "        wt_plus_one_prime = (env.wt*env.phi(k_t_plus_one_c)) / (np.dot(env.wt,env.phi(k_t_plus_one_c)))\n",
    "        \n",
    "        if np.random.rand() < epsilon: # goal is to take uniformly random action among possible actions of this env\n",
    "            possible_actions = env.F(p_t_plus_one_prime, wt_plus_one_prime) # when we sell/buy it's new prices, not protfolio\n",
    "            # value when action A has been taken, so when action A has been taken and then new state has come\n",
    "\n",
    "            random_index = np.random.randint(len(possible_actions))\n",
    "            action = env.actions[possible_actions[random_index]]\n",
    "        else: # here goal is to make model predict best action, but if action isn't feasable for env, map it\n",
    "            Q_values = model.predict((state['wt_prime'][np.newaxis], state['big_xt'][0][np.newaxis], state['big_xt'][1][np.newaxis], state['big_xt'][2][np.newaxis]))\n",
    "            action = env.action_mapping(env.actions[np.argmax(Q_values[0])], Q_values[0], p_t_plus_one_prime, wt_plus_one_prime)\n",
    "        \n",
    "        # simulate all feasible actions for current state\n",
    "        possible_actions = env.F(p_t_plus_one_prime, wt_plus_one_prime)\n",
    "        simulations = []\n",
    "        \n",
    "        for simulated_action in possible_actions:\n",
    "            next_state_simulated, next_reward, done, next_pt, next_wt = env.step(env.actions[simulated_action], simulation=True)\n",
    "\n",
    "            simulations.append((state['big_xt'], state['wt_prime'], simulated_action, next_reward, next_state_simulated['big_xt'], done, next_pt, np.array(next_wt)))\n",
    "        replay_memory.append(np.array(simulations))\n",
    "        next_state, reward, episode_ended = env.step(action)\n",
    "        # do some stuff with it\n",
    "        \n",
    "        # add some time for buffer to populate\n",
    "        \n",
    "        if steps >= 50:\n",
    "            # start batch training (room to improvment, why don't use Prioritized Experience Replay ?)\n",
    "            # select a uniformly random batch\n",
    "            indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "            batch = [replay_memory[index] for index in indices]\n",
    "            for simulations in batch:\n",
    "                #start = time.time()\n",
    "                preprocessed_states, wts_prime, actions, rewards, preprocessed_next_states, dones, next_pts, next_wts = [np.array([simulation[field_index] for simulation in simulations]) for field_index in range(8)]\n",
    "                \n",
    "                next_Q_values = target.predict((next_wts, preprocessed_next_states[:,0], preprocessed_next_states[:,1], preprocessed_next_states[:,2])) # Q-values predicted for all next_states\n",
    "                max_next_Q_values_index = np.argmax(next_Q_values, axis=1) # for each next_states find index of max Q-value predicted by target\n",
    "                \n",
    "                # okay, here, in max_next_Q_values, there are some actions that may not be feasible and that we need to map\n",
    "                # but we have to use is_asset_shortage, is_cash_shortage and action_mapping with state of simulation\n",
    "                max_next_Q_values_actions = np.array([\n",
    "                    env.actions[action_simulated] if not env.is_asset_shortage(env.actions[action_simulated], next_pts[ind], next_wts[ind]) and not env.is_cash_shortage(env.actions[action_simulated], next_pts[ind], next_wts[ind])\n",
    "                    else env.action_mapping(env.actions[action_simulated], next_Q_values[ind], next_pts[ind], next_wts[ind])\n",
    "                    for ind, action_simulated in enumerate(max_next_Q_values_index)\n",
    "                ])\n",
    "                \n",
    "                # okay, we have the feasible actions selected by the target network, now we need to find their indexes to communicate with the network\n",
    "                max_next_Q_values_indexes = [env.find_action_index(action_simulated) for action_simulated in max_next_Q_values_actions]\n",
    "                # now we have to find the Q-values predicted by target network of these corrected actions\n",
    "                max_next_Q_values = [next_Q_values[ind][best_action_simulated] for ind, best_action_simulated in enumerate(max_next_Q_values_indexes)]\n",
    "\n",
    "                # and now, we can proprely calculate the target Q-value\n",
    "                target_Q_values = (rewards +\n",
    "                                   (1 - dones) * discount_rate * max_next_Q_values) # here problem with dones /!\\\n",
    "                target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "                mask = tf.one_hot(actions, env.action_shape)\n",
    "                \n",
    "                train_step(preprocessed_states, wts_prime, mask, target_Q_values)\n",
    "                # and we're done, amazing !!\n",
    "                #print('updates ::: {:.5f} ms / step'.format((time.time() - start) * 1000))\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "        steps += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if episode_ended:\n",
    "            break;\n",
    "        \n",
    "    episode_counter += 1\n",
    "    \n",
    "    previous_CR = round((env.pt-env.initial_pt)/env.initial_pt, 3)\n",
    "    train_CR[ind].append(previous_CR)\n",
    "    pbar.close()\n",
    "    \n",
    "    for test_env in test_envs:\n",
    "        test_state = test_env.reset()\n",
    "        # state 0 return\n",
    "        # but current_tick set to one\n",
    "        test_episode_ended = False\n",
    "        while not test_episode_ended:\n",
    "            # state 0\n",
    "            # Q-values of action from state 0 predicted\n",
    "            # algorithm only knows state 0 and want to predict best action and then state one happens\n",
    "            \n",
    "            test_Q_values = model.predict((test_state['wt_prime'][np.newaxis], test_state['big_xt'][0][np.newaxis], test_state['big_xt'][1][np.newaxis], test_state['big_xt'][2][np.newaxis]))\n",
    "            \n",
    "            # we want to get total value of portfolio and new portfolio weights regarding state 0 evolution\n",
    "            # so we enter the algorithm with an already predefined portfolio distribution\n",
    "            # then state 0 happens\n",
    "            # so here current_tick should 0, not one,\n",
    "            # to calculate total value of portfolio and new portfolio weights regarding state 0 evolution\n",
    "            test_k_t_plus_one_c = test_env.big_pt[test_env.current_tick,0,:,test_env.n-1]\n",
    "            test_p_t_plus_one_prime = test_env.pt * np.dot(test_env.wt,test_env.phi(test_k_t_plus_one_c))\n",
    "            test_wt_plus_one_prime = (test_env.wt*test_env.phi(test_k_t_plus_one_c)) / (np.dot(test_env.wt,test_env.phi(test_k_t_plus_one_c)))\n",
    "            \n",
    "            test_action = test_env.action_mapping(test_env.actions[np.argmax(test_Q_values[0])], test_Q_values[0], test_p_t_plus_one_prime, test_wt_plus_one_prime)\n",
    "            \n",
    "            # okay here, we have selected best action predicted after state 0 evolution\n",
    "            # and we want to evolve to state one\n",
    "            test_state, test_reward, test_episode_ended = test_env.step(test_action)\n",
    "        \n",
    "        test_previous_CR = round((test_env.pt-test_env.initial_pt)/test_env.initial_pt, 3)\n",
    "        test_CR.append(test_previous_CR)\n",
    "        \n",
    "    # update of target network\n",
    "    target.set_weights(model.get_weights()) \n",
    "    epsilon = epsilon*(1-(ind+1)/500) # max(1 - episode_counter / 400, 0.01)\n",
    "\n",
    "# In[ ]:\n",
    "import pickle as pkl\n",
    "pkl.dump(train_CR, open('train_CR.pkl', 'wb'))\n",
    "pkl.dump(test_CR, open('test_CR.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
